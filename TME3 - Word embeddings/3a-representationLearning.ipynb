{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & representation learning: Neural Embeddings, Text Classification\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **Bag of Word (BoW)** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## [Dataset](https://thome.isir.upmc.fr/classes/RITAL/json_pol)\n",
    "\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (see next)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models\n",
    "6. Pytorch first look: learn to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n",
      "\n",
      "Number of test reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Loading json\n",
    "with open(\"ressources/json_pol.txt\",encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    json_data = json.loads(data[0])\n",
    "    train = json_data[\"train\"]\n",
    "    test = json_data[\"test\"]\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter_train = Counter((x[1] for x in train))\n",
    "counter_test = Counter((x[1] for x in test))\n",
    "print(\"Number of train reviews : \", len(train))\n",
    "print(\"----> # of positive : \", counter_train[1])\n",
    "print(\"----> # of negative : \", counter_train[0])\n",
    "print(\"\")\n",
    "print(train[0])\n",
    "print(\"\")\n",
    "print(\"Number of test reviews : \",len(test))\n",
    "print(\"----> # of positive : \", counter_test[1])\n",
    "print(\"----> # of negative : \", counter_test[0])\n",
    "\n",
    "print(\"\")\n",
    "print(test[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: train a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gensim not installed yet\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- Word2vec est un algorithme d'entrainement auto supervisé : on apprend à faire de la classif en supprimant des mots du texte.\n",
    "- Modèles pré-entrainé sur un grand nombre de documents.\n",
    "- Similarité cosine : produit scalaire normalisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:07:57,753 : INFO : collecting all words and their counts\n",
      "2023-02-16 14:07:57,754 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-02-16 14:07:58,790 : INFO : PROGRESS: at sentence #10000, processed 2358544 words, keeping 155393 word types\n",
      "2023-02-16 14:07:59,857 : INFO : PROGRESS: at sentence #20000, processed 4675912 words, keeping 243050 word types\n",
      "2023-02-16 14:08:00,399 : INFO : collected 280617 word types from a corpus of 5844680 raw words and 25000 sentences\n",
      "2023-02-16 14:08:00,399 : INFO : Creating a fresh vocabulary\n",
      "2023-02-16 14:08:00,989 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 49345 unique words (17.58% of original 280617, drops 231272)', 'datetime': '2023-02-16T14:08:00.989523', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 14:08:00,990 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5517507 word corpus (94.40% of original 5844680, drops 327173)', 'datetime': '2023-02-16T14:08:00.990610', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 14:08:01,616 : INFO : deleting the raw counts dictionary of 280617 items\n",
      "2023-02-16 14:08:01,625 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2023-02-16 14:08:01,627 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4268608.194985565 word corpus (77.4%% of prior 5517507)', 'datetime': '2023-02-16T14:08:01.627845', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 14:08:02,725 : INFO : estimated required memory for 49345 words and 100 dimensions: 64148500 bytes\n",
      "2023-02-16 14:08:02,726 : INFO : resetting layer weights\n",
      "2023-02-16 14:08:02,757 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-02-16T14:08:02.757754', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2023-02-16 14:08:02,758 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 49345 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-16T14:08:02.758997', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2023-02-16 14:08:03,813 : INFO : EPOCH 0 - PROGRESS: at 13.20% examples, 563987 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:04,810 : INFO : EPOCH 0 - PROGRESS: at 27.75% examples, 592446 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:05,819 : INFO : EPOCH 0 - PROGRESS: at 41.71% examples, 592162 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:06,848 : INFO : EPOCH 0 - PROGRESS: at 54.93% examples, 583224 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:07,876 : INFO : EPOCH 0 - PROGRESS: at 69.84% examples, 588133 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:08,899 : INFO : EPOCH 0 - PROGRESS: at 83.34% examples, 583391 words/s, in_qsize 6, out_qsize 1\n",
      "2023-02-16 14:08:09,897 : INFO : EPOCH 0 - PROGRESS: at 98.18% examples, 590521 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:10,020 : INFO : EPOCH 0: training on 5844680 raw words (4269167 effective words) took 7.2s, 590940 effective words/s\n",
      "2023-02-16 14:08:11,035 : INFO : EPOCH 1 - PROGRESS: at 13.70% examples, 590424 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:12,046 : INFO : EPOCH 1 - PROGRESS: at 27.58% examples, 588630 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:13,046 : INFO : EPOCH 1 - PROGRESS: at 40.88% examples, 579722 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:14,057 : INFO : EPOCH 1 - PROGRESS: at 54.35% examples, 580252 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:08:15,088 : INFO : EPOCH 1 - PROGRESS: at 68.04% examples, 576163 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:16,088 : INFO : EPOCH 1 - PROGRESS: at 82.27% examples, 578740 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:08:17,086 : INFO : EPOCH 1 - PROGRESS: at 96.10% examples, 581724 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:17,350 : INFO : EPOCH 1: training on 5844680 raw words (4269594 effective words) took 7.3s, 583310 effective words/s\n",
      "2023-02-16 14:08:18,355 : INFO : EPOCH 2 - PROGRESS: at 13.03% examples, 560191 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:19,368 : INFO : EPOCH 2 - PROGRESS: at 27.58% examples, 588275 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:20,398 : INFO : EPOCH 2 - PROGRESS: at 39.99% examples, 563864 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:21,407 : INFO : EPOCH 2 - PROGRESS: at 52.57% examples, 557948 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:22,408 : INFO : EPOCH 2 - PROGRESS: at 68.04% examples, 576557 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:23,414 : INFO : EPOCH 2 - PROGRESS: at 87.02% examples, 613240 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:24,012 : INFO : EPOCH 2: training on 5844680 raw words (4268600 effective words) took 6.7s, 641104 effective words/s\n",
      "2023-02-16 14:08:25,021 : INFO : EPOCH 3 - PROGRESS: at 17.90% examples, 767846 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:26,027 : INFO : EPOCH 3 - PROGRESS: at 32.76% examples, 699693 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:27,040 : INFO : EPOCH 3 - PROGRESS: at 45.08% examples, 640233 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:28,048 : INFO : EPOCH 3 - PROGRESS: at 59.06% examples, 628453 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:29,059 : INFO : EPOCH 3 - PROGRESS: at 72.46% examples, 613523 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:30,069 : INFO : EPOCH 3 - PROGRESS: at 86.44% examples, 610203 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:08:31,079 : INFO : EPOCH 3 - PROGRESS: at 99.69% examples, 602314 words/s, in_qsize 2, out_qsize 1\n",
      "2023-02-16 14:08:31,093 : INFO : EPOCH 3: training on 5844680 raw words (4268297 effective words) took 7.1s, 603138 effective words/s\n",
      "2023-02-16 14:08:32,099 : INFO : EPOCH 4 - PROGRESS: at 12.56% examples, 539821 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:33,118 : INFO : EPOCH 4 - PROGRESS: at 25.32% examples, 538094 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:34,123 : INFO : EPOCH 4 - PROGRESS: at 38.21% examples, 543782 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:35,120 : INFO : EPOCH 4 - PROGRESS: at 50.70% examples, 542247 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:36,130 : INFO : EPOCH 4 - PROGRESS: at 63.41% examples, 541803 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:37,150 : INFO : EPOCH 4 - PROGRESS: at 77.18% examples, 544182 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:38,150 : INFO : EPOCH 4 - PROGRESS: at 90.06% examples, 544885 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:38,899 : INFO : EPOCH 4: training on 5844680 raw words (4267725 effective words) took 7.8s, 547155 effective words/s\n",
      "2023-02-16 14:08:38,901 : INFO : Word2Vec lifecycle event {'msg': 'training on 29223400 raw words (21343383 effective words) took 36.1s, 590564 effective words/s', 'datetime': '2023-02-16T14:08:38.901324', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2023-02-16 14:08:38,901 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=49345, vector_size=100, alpha=0.025>', 'datetime': '2023-02-16T14:08:38.901324', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2023-02-16 14:08:38,906 : INFO : collecting all words and their counts\n",
      "2023-02-16 14:08:38,908 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-02-16 14:08:39,869 : INFO : PROGRESS: at sentence #10000, processed 2358544 words, keeping 155393 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:08:40,870 : INFO : PROGRESS: at sentence #20000, processed 4675912 words, keeping 243050 word types\n",
      "2023-02-16 14:08:41,390 : INFO : collected 280617 word types from a corpus of 5844680 raw words and 25000 sentences\n",
      "2023-02-16 14:08:41,390 : INFO : Creating a fresh vocabulary\n",
      "2023-02-16 14:08:41,980 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 49345 unique words (17.58% of original 280617, drops 231272)', 'datetime': '2023-02-16T14:08:41.980534', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 14:08:41,988 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5517507 word corpus (94.40% of original 5844680, drops 327173)', 'datetime': '2023-02-16T14:08:41.988218', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 14:08:42,628 : INFO : deleting the raw counts dictionary of 280617 items\n",
      "2023-02-16 14:08:42,643 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2023-02-16 14:08:42,645 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4268608.194985565 word corpus (77.4%% of prior 5517507)', 'datetime': '2023-02-16T14:08:42.645768', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 14:08:43,748 : INFO : estimated required memory for 49345 words and 100 dimensions: 64148500 bytes\n",
      "2023-02-16 14:08:43,748 : INFO : resetting layer weights\n",
      "2023-02-16 14:08:43,809 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-02-16T14:08:43.809387', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2023-02-16 14:08:43,809 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 49345 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-16T14:08:43.809387', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2023-02-16 14:08:44,823 : INFO : EPOCH 0 - PROGRESS: at 3.37% examples, 148778 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:45,830 : INFO : EPOCH 0 - PROGRESS: at 6.77% examples, 148473 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:46,836 : INFO : EPOCH 0 - PROGRESS: at 10.35% examples, 149057 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:47,879 : INFO : EPOCH 0 - PROGRESS: at 14.23% examples, 151129 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:08:48,913 : INFO : EPOCH 0 - PROGRESS: at 17.90% examples, 151399 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:49,978 : INFO : EPOCH 0 - PROGRESS: at 21.60% examples, 150713 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:50,978 : INFO : EPOCH 0 - PROGRESS: at 25.32% examples, 151534 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:52,001 : INFO : EPOCH 0 - PROGRESS: at 28.89% examples, 152004 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:08:53,059 : INFO : EPOCH 0 - PROGRESS: at 32.63% examples, 151462 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:54,091 : INFO : EPOCH 0 - PROGRESS: at 36.18% examples, 151616 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:08:55,100 : INFO : EPOCH 0 - PROGRESS: at 40.00% examples, 152092 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:56,169 : INFO : EPOCH 0 - PROGRESS: at 43.66% examples, 151695 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:57,181 : INFO : EPOCH 0 - PROGRESS: at 47.23% examples, 152065 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:58,199 : INFO : EPOCH 0 - PROGRESS: at 50.85% examples, 152247 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:08:59,214 : INFO : EPOCH 0 - PROGRESS: at 54.56% examples, 152468 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:00,243 : INFO : EPOCH 0 - PROGRESS: at 58.73% examples, 153434 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:01,282 : INFO : EPOCH 0 - PROGRESS: at 62.28% examples, 153254 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:02,318 : INFO : EPOCH 0 - PROGRESS: at 66.10% examples, 153249 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:09:03,357 : INFO : EPOCH 0 - PROGRESS: at 70.02% examples, 153223 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:04,451 : INFO : EPOCH 0 - PROGRESS: at 74.14% examples, 153469 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:05,459 : INFO : EPOCH 0 - PROGRESS: at 78.10% examples, 154033 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:06,462 : INFO : EPOCH 0 - PROGRESS: at 81.77% examples, 153952 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:07,468 : INFO : EPOCH 0 - PROGRESS: at 85.01% examples, 153522 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:08,496 : INFO : EPOCH 0 - PROGRESS: at 88.55% examples, 153301 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:09,505 : INFO : EPOCH 0 - PROGRESS: at 92.00% examples, 153166 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:10,567 : INFO : EPOCH 0 - PROGRESS: at 96.10% examples, 153530 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:11,588 : INFO : EPOCH 0 - PROGRESS: at 99.84% examples, 153482 words/s, in_qsize 1, out_qsize 1\n",
      "2023-02-16 14:09:11,596 : INFO : EPOCH 0: training on 5844680 raw words (4269513 effective words) took 27.8s, 153694 effective words/s\n",
      "2023-02-16 14:09:12,603 : INFO : EPOCH 1 - PROGRESS: at 3.20% examples, 142487 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:13,605 : INFO : EPOCH 1 - PROGRESS: at 6.93% examples, 152669 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:14,636 : INFO : EPOCH 1 - PROGRESS: at 11.05% examples, 157421 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:15,643 : INFO : EPOCH 1 - PROGRESS: at 14.72% examples, 157349 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:16,652 : INFO : EPOCH 1 - PROGRESS: at 18.43% examples, 156963 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:17,696 : INFO : EPOCH 1 - PROGRESS: at 22.14% examples, 155775 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:18,752 : INFO : EPOCH 1 - PROGRESS: at 25.80% examples, 154816 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:19,851 : INFO : EPOCH 1 - PROGRESS: at 29.80% examples, 155093 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:09:20,882 : INFO : EPOCH 1 - PROGRESS: at 33.52% examples, 155462 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:21,943 : INFO : EPOCH 1 - PROGRESS: at 37.20% examples, 154867 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:09:22,952 : INFO : EPOCH 1 - PROGRESS: at 41.26% examples, 155649 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:23,952 : INFO : EPOCH 1 - PROGRESS: at 44.63% examples, 155182 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:09:25,022 : INFO : EPOCH 1 - PROGRESS: at 48.17% examples, 154545 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:09:26,023 : INFO : EPOCH 1 - PROGRESS: at 51.85% examples, 154829 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:27,041 : INFO : EPOCH 1 - PROGRESS: at 55.65% examples, 154723 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:28,073 : INFO : EPOCH 1 - PROGRESS: at 59.40% examples, 154672 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:29,171 : INFO : EPOCH 1 - PROGRESS: at 63.07% examples, 154381 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:30,192 : INFO : EPOCH 1 - PROGRESS: at 67.00% examples, 154430 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:31,248 : INFO : EPOCH 1 - PROGRESS: at 70.82% examples, 154218 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:32,249 : INFO : EPOCH 1 - PROGRESS: at 74.48% examples, 154075 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:33,262 : INFO : EPOCH 1 - PROGRESS: at 78.29% examples, 154198 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:34,268 : INFO : EPOCH 1 - PROGRESS: at 81.91% examples, 154112 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:35,295 : INFO : EPOCH 1 - PROGRESS: at 85.34% examples, 153820 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:09:36,305 : INFO : EPOCH 1 - PROGRESS: at 89.06% examples, 153981 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:37,360 : INFO : EPOCH 1 - PROGRESS: at 92.69% examples, 153861 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:09:38,375 : INFO : EPOCH 1 - PROGRESS: at 96.28% examples, 153692 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:39,329 : INFO : EPOCH 1: training on 5844680 raw words (4269381 effective words) took 27.7s, 153979 effective words/s\n",
      "2023-02-16 14:09:40,336 : INFO : EPOCH 2 - PROGRESS: at 3.37% examples, 149315 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:41,430 : INFO : EPOCH 2 - PROGRESS: at 6.94% examples, 146417 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:09:42,470 : INFO : EPOCH 2 - PROGRESS: at 10.68% examples, 148052 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:43,500 : INFO : EPOCH 2 - PROGRESS: at 14.56% examples, 150832 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:09:44,529 : INFO : EPOCH 2 - PROGRESS: at 18.09% examples, 149888 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:45,572 : INFO : EPOCH 2 - PROGRESS: at 23.27% examples, 160134 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:46,622 : INFO : EPOCH 2 - PROGRESS: at 27.75% examples, 163600 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:47,631 : INFO : EPOCH 2 - PROGRESS: at 31.28% examples, 161977 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:48,635 : INFO : EPOCH 2 - PROGRESS: at 34.82% examples, 161271 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:49,732 : INFO : EPOCH 2 - PROGRESS: at 38.74% examples, 160097 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:50,732 : INFO : EPOCH 2 - PROGRESS: at 42.35% examples, 159252 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:51,741 : INFO : EPOCH 2 - PROGRESS: at 45.90% examples, 159067 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:52,741 : INFO : EPOCH 2 - PROGRESS: at 49.53% examples, 158924 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:53,779 : INFO : EPOCH 2 - PROGRESS: at 53.28% examples, 158462 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:54,780 : INFO : EPOCH 2 - PROGRESS: at 56.90% examples, 157927 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:55,810 : INFO : EPOCH 2 - PROGRESS: at 60.50% examples, 157607 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:56,812 : INFO : EPOCH 2 - PROGRESS: at 64.09% examples, 157581 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:57,924 : INFO : EPOCH 2 - PROGRESS: at 68.21% examples, 157095 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:09:58,930 : INFO : EPOCH 2 - PROGRESS: at 71.94% examples, 156804 words/s, in_qsize 6, out_qsize 1\n",
      "2023-02-16 14:10:00,041 : INFO : EPOCH 2 - PROGRESS: at 76.00% examples, 156743 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:01,072 : INFO : EPOCH 2 - PROGRESS: at 79.83% examples, 156639 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:02,110 : INFO : EPOCH 2 - PROGRESS: at 83.52% examples, 156530 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:03,160 : INFO : EPOCH 2 - PROGRESS: at 87.36% examples, 156599 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:04,190 : INFO : EPOCH 2 - PROGRESS: at 91.01% examples, 156486 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:05,223 : INFO : EPOCH 2 - PROGRESS: at 94.82% examples, 156396 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:06,262 : INFO : EPOCH 2 - PROGRESS: at 98.55% examples, 156256 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:06,645 : INFO : EPOCH 2: training on 5844680 raw words (4268148 effective words) took 27.3s, 156273 effective words/s\n",
      "2023-02-16 14:10:07,652 : INFO : EPOCH 3 - PROGRESS: at 3.37% examples, 149177 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:08,762 : INFO : EPOCH 3 - PROGRESS: at 6.93% examples, 145169 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:09,772 : INFO : EPOCH 3 - PROGRESS: at 10.85% examples, 150686 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:10,796 : INFO : EPOCH 3 - PROGRESS: at 14.56% examples, 151655 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:11,832 : INFO : EPOCH 3 - PROGRESS: at 18.27% examples, 151578 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:12,950 : INFO : EPOCH 3 - PROGRESS: at 22.28% examples, 151909 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:14,032 : INFO : EPOCH 3 - PROGRESS: at 26.28% examples, 153061 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:15,033 : INFO : EPOCH 3 - PROGRESS: at 29.80% examples, 152764 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:16,052 : INFO : EPOCH 3 - PROGRESS: at 33.36% examples, 152869 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:17,101 : INFO : EPOCH 3 - PROGRESS: at 37.20% examples, 153264 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:18,102 : INFO : EPOCH 3 - PROGRESS: at 40.88% examples, 153005 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:19,142 : INFO : EPOCH 3 - PROGRESS: at 44.50% examples, 152879 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:20,145 : INFO : EPOCH 3 - PROGRESS: at 47.86% examples, 152682 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:21,163 : INFO : EPOCH 3 - PROGRESS: at 51.49% examples, 152926 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:22,195 : INFO : EPOCH 3 - PROGRESS: at 55.28% examples, 152832 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:23,213 : INFO : EPOCH 3 - PROGRESS: at 59.24% examples, 153465 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:24,218 : INFO : EPOCH 3 - PROGRESS: at 62.57% examples, 153197 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:25,252 : INFO : EPOCH 3 - PROGRESS: at 66.50% examples, 153151 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:26,268 : INFO : EPOCH 3 - PROGRESS: at 70.34% examples, 153362 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:27,321 : INFO : EPOCH 3 - PROGRESS: at 74.14% examples, 153233 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:28,361 : INFO : EPOCH 3 - PROGRESS: at 77.88% examples, 153231 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:29,365 : INFO : EPOCH 3 - PROGRESS: at 81.60% examples, 153183 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:30,399 : INFO : EPOCH 3 - PROGRESS: at 85.32% examples, 153495 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:31,408 : INFO : EPOCH 3 - PROGRESS: at 89.06% examples, 153666 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:32,464 : INFO : EPOCH 3 - PROGRESS: at 92.69% examples, 153518 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:33,471 : INFO : EPOCH 3 - PROGRESS: at 96.44% examples, 153701 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:34,430 : INFO : EPOCH 3: training on 5844680 raw words (4269130 effective words) took 27.8s, 153696 effective words/s\n",
      "2023-02-16 14:10:35,522 : INFO : EPOCH 4 - PROGRESS: at 3.54% examples, 144431 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:36,632 : INFO : EPOCH 4 - PROGRESS: at 7.46% examples, 149425 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:37,632 : INFO : EPOCH 4 - PROGRESS: at 11.22% examples, 151661 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:38,712 : INFO : EPOCH 4 - PROGRESS: at 15.05% examples, 152024 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:39,762 : INFO : EPOCH 4 - PROGRESS: at 18.92% examples, 152776 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:40,789 : INFO : EPOCH 4 - PROGRESS: at 22.61% examples, 152819 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:41,812 : INFO : EPOCH 4 - PROGRESS: at 26.16% examples, 152085 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:42,812 : INFO : EPOCH 4 - PROGRESS: at 29.96% examples, 153609 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:43,822 : INFO : EPOCH 4 - PROGRESS: at 33.36% examples, 152991 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:44,843 : INFO : EPOCH 4 - PROGRESS: at 37.01% examples, 153170 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:45,850 : INFO : EPOCH 4 - PROGRESS: at 40.88% examples, 153504 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:46,912 : INFO : EPOCH 4 - PROGRESS: at 44.50% examples, 152999 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:47,942 : INFO : EPOCH 4 - PROGRESS: at 48.17% examples, 153472 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:49,012 : INFO : EPOCH 4 - PROGRESS: at 51.85% examples, 153173 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:10:50,042 : INFO : EPOCH 4 - PROGRESS: at 55.65% examples, 153096 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:51,059 : INFO : EPOCH 4 - PROGRESS: at 59.40% examples, 153284 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:52,072 : INFO : EPOCH 4 - PROGRESS: at 62.87% examples, 153335 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:53,122 : INFO : EPOCH 4 - PROGRESS: at 66.85% examples, 153228 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:54,122 : INFO : EPOCH 4 - PROGRESS: at 70.64% examples, 153471 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:55,182 : INFO : EPOCH 4 - PROGRESS: at 74.48% examples, 153270 words/s, in_qsize 6, out_qsize 0\n",
      "2023-02-16 14:10:56,201 : INFO : EPOCH 4 - PROGRESS: at 78.10% examples, 153123 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:57,260 : INFO : EPOCH 4 - PROGRESS: at 81.94% examples, 153018 words/s, in_qsize 4, out_qsize 1\n",
      "2023-02-16 14:10:58,282 : INFO : EPOCH 4 - PROGRESS: at 85.65% examples, 153416 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:10:59,341 : INFO : EPOCH 4 - PROGRESS: at 89.23% examples, 152999 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:11:00,342 : INFO : EPOCH 4 - PROGRESS: at 92.69% examples, 152921 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:11:01,362 : INFO : EPOCH 4 - PROGRESS: at 96.44% examples, 153025 words/s, in_qsize 5, out_qsize 0\n",
      "2023-02-16 14:11:02,272 : INFO : EPOCH 4: training on 5844680 raw words (4268492 effective words) took 27.8s, 153315 effective words/s\n",
      "2023-02-16 14:11:02,279 : INFO : Word2Vec lifecycle event {'msg': 'training on 29223400 raw words (21344664 effective words) took 138.5s, 154151 effective words/s', 'datetime': '2023-02-16T14:11:02.278470', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2023-02-16 14:11:02,280 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=49345, vector_size=100, alpha=0.025>', 'datetime': '2023-02-16T14:11:02.280548', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in train]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v_cbow = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=0, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)\n",
    "\n",
    "w2v_sg = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:11:24,833 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'W2v_cbow-movies.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-16T14:11:24.833059', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'saving'}\n",
      "2023-02-16 14:11:24,836 : INFO : not storing attribute cum_table\n",
      "2023-02-16 14:11:24,924 : INFO : saved W2v_cbow-movies.dat\n",
      "2023-02-16 14:11:24,925 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'W2v_sg-movies.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-16T14:11:24.925150', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'saving'}\n",
      "2023-02-16 14:11:24,926 : INFO : not storing attribute cum_table\n",
      "2023-02-16 14:11:25,012 : INFO : saved W2v_sg-movies.dat\n"
     ]
    }
   ],
   "source": [
    "# Worth it to save the previous embedding\n",
    "w2v.save(\"W2v_cbow-movies.dat\")\n",
    "w2v.save(\"W2v_sg-movies.dat\")\n",
    "# You will be able to reload them:\n",
    "# w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\")\n",
    "# and you can continue the learning process if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.79202557\n",
      "great and bad: 0.4801622\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v_cbow.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v_sg.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9370229244232178),\n",
       " ('movie,', 0.8480282425880432),\n",
       " ('film,', 0.7691652774810791),\n",
       " ('flick', 0.7553712129592896),\n",
       " ('picture', 0.7163830995559692)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "#w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "#w2v.wv.most_similar(\"actor\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('author,', 0.795407235622406),\n",
       " ('forceful', 0.7824472784996033),\n",
       " ('internationally', 0.7710718512535095)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "# w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "# w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "\n",
    "\n",
    "# Try other things like plurals for exemple.\n",
    "w2v.wv.most_similar(positive=[\"angel\",\"angels\"],negative=[\"car\"],topn=3) # do the famous exemple works for actor ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:11:49,823 : INFO : Evaluating word analogies for top 300000 words in the model on ressources/questions-words.txt\n",
      "2023-02-16 14:11:50,298 : INFO : capital-common-countries: 1.9% (3/156)\n",
      "2023-02-16 14:11:50,648 : INFO : capital-world: 0.0% (0/111)\n",
      "2023-02-16 14:11:50,710 : INFO : currency: 0.0% (0/18)\n",
      "2023-02-16 14:11:51,619 : INFO : city-in-state: 0.0% (0/301)\n",
      "2023-02-16 14:11:52,936 : INFO : family: 28.6% (120/420)\n",
      "2023-02-16 14:11:55,559 : INFO : gram1-adjective-to-adverb: 0.9% (8/870)\n",
      "2023-02-16 14:11:57,231 : INFO : gram2-opposite: 2.7% (15/552)\n",
      "2023-02-16 14:12:00,823 : INFO : gram3-comparative: 22.4% (266/1190)\n",
      "2023-02-16 14:12:03,047 : INFO : gram4-superlative: 6.0% (45/756)\n",
      "2023-02-16 14:12:05,457 : INFO : gram5-present-participle: 15.0% (122/812)\n",
      "2023-02-16 14:12:08,214 : INFO : gram6-nationality-adjective: 1.2% (12/967)\n",
      "2023-02-16 14:12:11,768 : INFO : gram7-past-tense: 16.7% (210/1260)\n",
      "2023-02-16 14:12:14,038 : INFO : gram8-plural: 6.3% (51/812)\n",
      "2023-02-16 14:12:15,857 : INFO : gram9-plural-verbs: 22.5% (146/650)\n",
      "2023-02-16 14:12:15,857 : INFO : Quadruplets with out-of-vocabulary words: 54.6%\n",
      "2023-02-16 14:12:15,866 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2023-02-16 14:12:15,868 : INFO : Total accuracy: 11.2% (998/8875)\n"
     ]
    }
   ],
   "source": [
    "out = w2v_cbow.wv.evaluate_word_analogies(\"ressources/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:12:38,821 : INFO : Evaluating word analogies for top 300000 words in the model on ressources/questions-words.txt\n",
      "2023-02-16 14:12:39,270 : INFO : capital-common-countries: 2.6% (4/156)\n",
      "2023-02-16 14:12:39,637 : INFO : capital-world: 1.8% (2/111)\n",
      "2023-02-16 14:12:39,707 : INFO : currency: 0.0% (0/18)\n",
      "2023-02-16 14:12:40,688 : INFO : city-in-state: 0.0% (0/301)\n",
      "2023-02-16 14:12:41,869 : INFO : family: 34.3% (144/420)\n",
      "2023-02-16 14:12:44,138 : INFO : gram1-adjective-to-adverb: 1.7% (15/870)\n",
      "2023-02-16 14:12:45,699 : INFO : gram2-opposite: 3.1% (17/552)\n",
      "2023-02-16 14:12:49,246 : INFO : gram3-comparative: 18.6% (221/1190)\n",
      "2023-02-16 14:12:51,548 : INFO : gram4-superlative: 9.9% (75/756)\n",
      "2023-02-16 14:12:53,975 : INFO : gram5-present-participle: 20.9% (170/812)\n",
      "2023-02-16 14:12:56,738 : INFO : gram6-nationality-adjective: 1.4% (14/967)\n",
      "2023-02-16 14:13:00,303 : INFO : gram7-past-tense: 21.2% (267/1260)\n",
      "2023-02-16 14:13:02,493 : INFO : gram8-plural: 6.4% (52/812)\n",
      "2023-02-16 14:13:04,259 : INFO : gram9-plural-verbs: 30.3% (197/650)\n",
      "2023-02-16 14:13:04,259 : INFO : Quadruplets with out-of-vocabulary words: 54.6%\n",
      "2023-02-16 14:13:04,264 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2023-02-16 14:13:04,265 : INFO : Total accuracy: 13.3% (1178/8875)\n"
     ]
    }
   ],
   "source": [
    "out = w2v_sg.wv.evaluate_word_analogies(\"ressources/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data (and thus it hasen't seen a lot of words), it does not perform very well.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:16:45,195 : INFO : loading projection weights from C:\\Users\\PC/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2023-02-16 14:18:35,618 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\PC/gensim-data\\\\word2vec-google-news-300\\\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-02-16T14:18:35.618111', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'load_word2vec_format'}\n",
      "2023-02-16 14:18:35,618 : INFO : KeyedVectors lifecycle event {'fname_or_handle': 'ressources/word2vec-google-news-300.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-16T14:18:35.618111', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'saving'}\n",
      "2023-02-16 14:18:35,626 : INFO : storing np array 'vectors' to ressources/word2vec-google-news-300.dat.vectors.npy\n",
      "2023-02-16 14:18:49,296 : INFO : saved ressources/word2vec-google-news-300.dat\n"
     ]
    }
   ],
   "source": [
    "#from gensim.test.utils import get_tmpfile\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "bload = False\n",
    "# A word2vec trained on google news and of dimension 300\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"ressources/\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the \"synctactic\" and \"semantic\" evaluations again. Conclude on the pre-trained embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 14:20:09,547 : INFO : Evaluating word analogies for top 300000 words in the model on ressources/questions-words.txt\n",
      "2023-02-16 14:20:31,949 : INFO : capital-common-countries: 83.2% (421/506)\n",
      "2023-02-16 14:22:53,409 : INFO : capital-world: 81.3% (3552/4368)\n",
      "2023-02-16 14:23:19,176 : INFO : currency: 28.5% (230/808)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mwv_pre_trained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_word_analogies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mressources/questions-words.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcase_insensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1371\u001b[0m, in \u001b[0;36mKeyedVectors.evaluate_word_analogies\u001b[1;34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001b[0m\n\u001b[0;32m   1367\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;66;03m# find the most likely prediction using 3CosAdd (vector offset) method\u001b[39;00m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;66;03m# TODO: implement 3CosMul and set-based methods for solving analogies\u001b[39;00m\n\u001b[1;32m-> 1371\u001b[0m sims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestrict_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestrict_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_to_index \u001b[38;5;241m=\u001b[39m original_key_to_index\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m sims:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py:852\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m topn:\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dists\n\u001b[1;32m--> 852\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mmatutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_keys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# ignore (don't return) keys from the input\u001b[39;00m\n\u001b[0;32m    854\u001b[0m result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    855\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_key[sim \u001b[38;5;241m+\u001b[39m clip_start], \u001b[38;5;28mfloat\u001b[39m(dists[sim]))\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sim \u001b[38;5;129;01min\u001b[39;00m best \u001b[38;5;28;01mif\u001b[39;00m (sim \u001b[38;5;241m+\u001b[39m clip_start) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_keys\n\u001b[0;32m    857\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\matutils.py:77\u001b[0m, in \u001b[0;36margsort\u001b[1;34m(x, topn, reverse)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(x)[:topn]\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# np >= 1.8 has a fast partial argsort, use that!\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m most_extreme \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m)\u001b[49m[:topn]\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m most_extreme\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margsort(x\u001b[38;5;241m.\u001b[39mtake(most_extreme)))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margpartition\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:845\u001b[0m, in \u001b[0;36margpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    843\u001b[0m \n\u001b[0;32m    844\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = wv_pre_trained.evaluate_word_analogies(\"ressources/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une des différences avec les transformers c'est l'approche attentionnelle\n",
    "\n",
    "- Un token cls\n",
    "- Un pooling plus fin\n",
    "- Moyenne pondérée avec tout les mots de la phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# wv_pre_trained.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv_pre_trained.vectors\n",
    "# wv_pre_trained.get_vector('with')\n",
    "# wv_pre_trained['with']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize(words, model, method='sum'):\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vectors.append(model[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    if method == 'sum':\n",
    "        return np.sum(vectors, axis=0)\n",
    "    \n",
    "    elif method == 'average':\n",
    "        return np.mean(vectors, axis=0)\n",
    "    \n",
    "    elif method == 'min':\n",
    "        return np.min(vectors, axis=0)\n",
    "    \n",
    "    elif method == 'max':\n",
    "        return np.max(vectors, axis=0)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid method, choose one of 'sum', 'average', 'min', 'max'\")\n",
    "\n",
    "# ### TEST   \n",
    "# classes = [pol for text,pol in train]\n",
    "# X = [vectorize(text, wv_pre_trained) for text,pol in train]\n",
    "# X_test = [vectorize(text, wv_pre_trained) for text,pol in test]\n",
    "# true = [pol for text,pol in test]\n",
    "\n",
    "# #let's see what a review vector looks like.\n",
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier using a trained model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      " 25%|████████████████████▊                                                              | 1/4 [03:54<11:42, 234.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method : sum\n",
      "Test accuracy: 0.64572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 2/4 [07:50<07:50, 235.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method : average\n",
      "Test accuracy: 0.594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\r",
      " 75%|██████████████████████████████████████████████████████████████▎                    | 3/4 [11:59<04:01, 241.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method : min\n",
      "Test accuracy: 0.58904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [16:01<00:00, 240.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method : max\n",
      "Test accuracy: 0.59232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word_vectors = w2v_cbow.wv # your pre-trained word embedding model\n",
    "\n",
    "# Load your sentiment data\n",
    "y_train = [c for r,c in train]\n",
    "y_test  = [c for r,c in test ]\n",
    "\n",
    "methods = ['sum', 'average', 'min', 'max']\n",
    "\n",
    "for m in tqdm(methods):\n",
    "    # Vectorize the input data\n",
    "    X_train = [vectorize(text, word_vectors, method=m) for text,pol in train]\n",
    "    X_test  = [vectorize(text, word_vectors, method=m) for text,pol in test]\n",
    "    \n",
    "    # Train a logistic regression model\n",
    "    clf = LogisticRegression(max_iter=500)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "    # Results\n",
    "    print('Method :', m)\n",
    "    print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier using the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      " 25%|████████████████████▊                                                              | 1/4 [03:17<09:53, 197.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time :  13.9268639087677\n",
      "Method : sum\n",
      "Test accuracy: 0.58756\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 2/4 [06:19<06:16, 188.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time :  2.244309663772583\n",
      "Method : average\n",
      "Test accuracy: 0.58528\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████▎                    | 3/4 [09:18<03:04, 184.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time :  3.518094539642334\n",
      "Method : min\n",
      "Test accuracy: 0.57396\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [12:19<00:00, 184.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time :  4.330111742019653\n",
      "Method : max\n",
      "Test accuracy: 0.57848\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word_vectors = wv_pre_trained # your pre-trained word embedding model\n",
    "\n",
    "# Load your sentiment data\n",
    "y_train = [c for r,c in train]\n",
    "y_test  = [c for r,c in test ]\n",
    "\n",
    "methods = ['sum', 'average', 'min', 'max']\n",
    "    \n",
    "for m in tqdm(methods):\n",
    "    # Vectorize the input data\n",
    "    X_train = [vectorize(text, wv_pre_trained, method=m) for text,pol in train]\n",
    "    X_test  = [vectorize(text, wv_pre_trained, method=m) for text,pol in test]\n",
    "    \n",
    "    # Train a logistic regression model\n",
    "    tic = time.time()\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "    # Results\n",
    "    print('Exec time : ', time.time()-tic)\n",
    "    print('Method :', m)\n",
    "    print(\"Test accuracy:\", accuracy)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time :  1261.5267460346222\n",
      "Method : sum\n",
      "Test accuracy: 0.64572\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word_vectors = w2v_cbow.wv # your pre-trained word embedding model\n",
    "\n",
    "# Load your sentiment data\n",
    "y_train = [c for r,c in train]\n",
    "y_test  = [c for r,c in test ]\n",
    "\n",
    "m = 'sum'\n",
    "\n",
    "# Vectorize the input data\n",
    "X_train = [vectorize(text, word_vectors, method=m) for text,pol in train]\n",
    "X_test  = [vectorize(text, word_vectors, method=m) for text,pol in test]\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "# Results\n",
    "print('Exec time : ', time.time()-tic)\n",
    "print('Method :', m)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time :  1393.2965927124023\n",
      "Method : sum\n",
      "Test accuracy: 0.6532\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word_vectors = w2v_sg.wv # your pre-trained word embedding model\n",
    "\n",
    "# Load your sentiment data\n",
    "y_train = [c for r,c in train]\n",
    "y_test  = [c for r,c in test ]\n",
    "\n",
    "m = 'sum'\n",
    "\n",
    "# Vectorize the input data\n",
    "X_train = [vectorize(text, word_vectors, method=m) for text,pol in train]\n",
    "X_test  = [vectorize(text, word_vectors, method=m) for text,pol in test]\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "# Results\n",
    "print('Exec time : ', time.time()-tic)\n",
    "print('Method :', m)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "\n",
    "def vectorize_tfidf(docs, model):\n",
    "    # Create a TfidfVectorizer objecté\n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    # Compute the tf-idf weights for the documents\n",
    "    tfidf_weights = tfidf.fit_transform(docs)\n",
    "\n",
    "    # Get the vocabulary and idf weights from the TfidfVectorizer\n",
    "    vocabulary = tfidf.vocabulary_\n",
    "    idf_weights = tfidf.idf_\n",
    "\n",
    "    # Create an empty numpy array to store the aggregated vectors\n",
    "    agg_vectors = np.zeros((len(docs), model.vector_size))\n",
    "\n",
    "    # Loop over the documents and aggregate the vectors\n",
    "    for i, doc in enumerate(docs):\n",
    "        # Split the document into individual words\n",
    "        words = doc.split()\n",
    "\n",
    "        # Loop over the words and aggregate their vectors\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                # Compute the tf-idf weight for the word\n",
    "                tf_idf = idf_weights[vocabulary[word]] * (words.count(word) / len(words))\n",
    "\n",
    "                # Add the weighted vector for the word to the aggregate vector for the document\n",
    "                agg_vectors[i] += tf_idf * model[word]\n",
    "\n",
    "    return agg_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word_vectors = w2v_sg.wv # your pre-trained word embedding model\n",
    "\n",
    "# Load your sentiment data\n",
    "y_train = [c for r,c in train]\n",
    "y_test  = [c for r,c in test ]\n",
    "\n",
    "# Vectorize the input data\n",
    "X_train = [vectorize_tfidf(text, word_vectors) for text,pol in train]\n",
    "X_test  = [vectorize_tfidf(text, word_vectors) for text,pol in test]\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "# Results\n",
    "print('Exec time    : ', time.time()-tic)\n",
    "print('Method       :', m)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 15:53:03,233 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2023-02-16 15:53:03,235 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\n",
      "2023-02-16 15:53:03,236 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\", 'datetime': '2023-02-16T15:53:03.236808', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2023-02-16 15:53:03,535 : INFO : collecting all words and their counts\n",
      "2023-02-16 15:53:03,536 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2023-02-16 15:53:03,603 : INFO : PROGRESS: at example #10000, processed 20000 words (313453 words/s), 9985 word types, 0 tags\n",
      "2023-02-16 15:53:03,665 : INFO : PROGRESS: at example #20000, processed 40000 words (326753 words/s), 19949 word types, 0 tags\n",
      "2023-02-16 15:53:03,701 : INFO : collected 24906 word types and 25000 unique tags from a corpus of 25000 examples and 50000 words\n",
      "2023-02-16 15:53:03,701 : INFO : Creating a fresh vocabulary\n",
      "2023-02-16 15:53:04,001 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 24906 unique words (100.00% of original 24906, drops 0)', 'datetime': '2023-02-16T15:53:04.001934', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 15:53:04,002 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 50000 word corpus (100.00% of original 50000, drops 0)', 'datetime': '2023-02-16T15:53:04.002972', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 15:53:04,526 : INFO : deleting the raw counts dictionary of 24906 items\n",
      "2023-02-16 15:53:04,529 : INFO : sample=0.001 downsamples 2 most-common words\n",
      "2023-02-16 15:53:04,530 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 26681.13883008419 word corpus (53.4%% of prior 50000)', 'datetime': '2023-02-16T15:53:04.530037', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-16 15:53:05,413 : INFO : estimated required memory for 24906 words and 5 dimensions: 18949240 bytes\n",
      "2023-02-16 15:53:05,414 : INFO : resetting layer weights\n",
      "2023-02-16 15:53:05,459 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 24906 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-16T15:53:05.459069', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2023-02-16 15:53:10,310 : INFO : EPOCH 0 - PROGRESS: at 20.00% examples, 2134 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:11,233 : INFO : EPOCH 0: training on 50000 raw words (51637 effective words) took 5.8s, 8976 effective words/s\n",
      "2023-02-16 15:53:16,170 : INFO : EPOCH 1 - PROGRESS: at 20.00% examples, 2107 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:16,778 : INFO : EPOCH 1: training on 50000 raw words (51797 effective words) took 5.5s, 9367 effective words/s\n",
      "2023-02-16 15:53:21,451 : INFO : EPOCH 2 - PROGRESS: at 20.00% examples, 2219 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:22,505 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 9046 words/s, in_qsize 0, out_qsize 1\n",
      "2023-02-16 15:53:22,506 : INFO : EPOCH 2: training on 50000 raw words (51700 effective words) took 5.7s, 9044 effective words/s\n",
      "2023-02-16 15:53:27,321 : INFO : EPOCH 3 - PROGRESS: at 20.00% examples, 2150 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:27,989 : INFO : EPOCH 3: training on 50000 raw words (51661 effective words) took 5.5s, 9446 effective words/s\n",
      "2023-02-16 15:53:32,684 : INFO : EPOCH 4 - PROGRESS: at 20.00% examples, 2212 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:33,312 : INFO : EPOCH 4: training on 50000 raw words (51680 effective words) took 5.3s, 9742 effective words/s\n",
      "2023-02-16 15:53:38,032 : INFO : EPOCH 5 - PROGRESS: at 20.00% examples, 2198 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:38,817 : INFO : EPOCH 5: training on 50000 raw words (51708 effective words) took 5.5s, 9411 effective words/s\n",
      "2023-02-16 15:53:43,684 : INFO : EPOCH 6 - PROGRESS: at 20.00% examples, 2129 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:44,311 : INFO : EPOCH 6: training on 50000 raw words (51665 effective words) took 5.5s, 9424 effective words/s\n",
      "2023-02-16 15:53:48,751 : INFO : EPOCH 7 - PROGRESS: at 20.00% examples, 2329 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:49,488 : INFO : EPOCH 7: training on 50000 raw words (51667 effective words) took 5.2s, 10011 effective words/s\n",
      "2023-02-16 15:53:54,606 : INFO : EPOCH 8 - PROGRESS: at 20.00% examples, 2023 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:53:55,150 : INFO : EPOCH 8: training on 50000 raw words (51684 effective words) took 5.6s, 9154 effective words/s\n",
      "2023-02-16 15:54:00,652 : INFO : EPOCH 9 - PROGRESS: at 20.00% examples, 1883 words/s, in_qsize 4, out_qsize 0\n",
      "2023-02-16 15:54:01,171 : INFO : EPOCH 9: training on 50000 raw words (51686 effective words) took 6.0s, 8606 effective words/s\n",
      "2023-02-16 15:54:01,174 : INFO : Doc2Vec lifecycle event {'msg': 'training on 500000 raw words (516885 effective words) took 55.7s, 9278 effective words/s', 'datetime': '2023-02-16T15:54:01.174180', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2023-02-16 15:54:01,174 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d5,n5,w2,s0.001,t4>', 'datetime': '2023-02-16T15:54:01.174180', 'gensim': '4.3.0', 'python': '3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe\n",
    "\n",
    "sentences: This is the input data used to train the Word2Vec model. In this case, it is a list of sentences represented by the text variable.\n",
    "\n",
    "vector_size: This parameter sets the size of the vector used to represent each word. In this case, it is set to 100.\n",
    "\n",
    "window: This parameter sets the size of the window used to predict the target word given the context words. It is set to 5, which means that the model will consider the five words before and after the target word.\n",
    "\n",
    "min_count: This parameter sets the minimum frequency of a word that should be included in the vocabulary. Words that occur less frequently than min_count times in the input data will be ignored. It is set to 5 in this case.\n",
    "\n",
    "sample: This parameter sets the threshold for downsampling high-frequency words. Words that occur more frequently than sample will be randomly downsampled. It is set to 0.001 in this case.\n",
    "\n",
    "workers: This parameter sets the number of worker threads to use for training the model. It is set to 3 in this case.\n",
    "\n",
    "sg: This parameter sets the training algorithm. It is set to 1, which means that the model will use the Skip-gram (SG) algorithm instead of the default Continuous Bag of Words (CBOW) algorithm.\n",
    "\n",
    "hs: This parameter sets the hierarchical softmax training algorithm. It is set to 0, which means that negative sampling will be used instead of hierarchical softmax.\n",
    "\n",
    "negative: This parameter sets the number of negative samples to use when training the model. It is set to 5 in this case.\n",
    "\n",
    "cbow_mean: This parameter sets the method for computing the word vectors in the CBOW algorithm. It is set to 1, which means that the vectors will be the mean of the context vectors.\n",
    "\n",
    "epochs: This parameter sets the number of epochs (iterations) to train the model. It is set to 5 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
