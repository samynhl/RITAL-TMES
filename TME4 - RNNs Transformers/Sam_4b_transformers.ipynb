{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Warning : \n",
        "# Do \"File -> Save a copy in Drive\" before you start modifying the notebook, otherwise your modifications will not be saved."
      ],
      "metadata": {
        "id": "YPB2EK9FEiMp"
      },
      "id": "YPB2EK9FEiMp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT for Sentiment Analysis "
      ],
      "metadata": {
        "id": "_UFwj3KNufA4"
      },
      "id": "_UFwj3KNufA4"
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers"
      ],
      "metadata": {
        "id": "IlFyycx9qLTP"
      },
      "id": "IlFyycx9qLTP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6e14ce48",
      "metadata": {
        "id": "6e14ce48"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading large review movie dataset (50000 reviews in train, 50000 reviews in test)"
      ],
      "metadata": {
        "id": "BtKn-7auvbsh"
      },
      "id": "BtKn-7auvbsh"
    },
    {
      "cell_type": "code",
      "source": [
        "# wget https://thome.isir.upmc.fr/classes/RITAL/json_pol"
      ],
      "metadata": {
        "id": "a5i1H9--qZsC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "1be6c786-244e-4d1f-cdfc-5b0a79bc890c"
      },
      "id": "a5i1H9--qZsC",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0c4ac5014802>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    wget https://thome.isir.upmc.fr/classes/RITAL/json_pol\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "url = 'https://thome.isir.upmc.fr/classes/RITAL/json_pol'\n",
        "filename = 'json_pol'\n",
        "\n",
        "urllib.request.urlretrieve(url, filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jYajthMeUbx",
        "outputId": "b2bd8714-9f20-4651-ddc4-7708abe0ad1b"
      },
      "id": "1jYajthMeUbx",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('json_pol', <http.client.HTTPMessage at 0x7f734c4885e0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6c40511e",
      "metadata": {
        "id": "6c40511e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e987889d-cebf-49e5-96e4-a7cb079e2edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train reviews :  25000\n",
            "----> # of positive :  12500\n",
            "----> # of negative :  12500\n",
            "\n",
            "[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n",
            "\n",
            "Number of test reviews :  25000\n",
            "----> # of positive :  12500\n",
            "----> # of negative :  12500\n",
            "\n",
            "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Loading json\n",
        "with open(\"./json_pol\",encoding=\"utf-8\") as f:\n",
        "    data = f.readlines()\n",
        "    json_data = json.loads(data[0])\n",
        "    train = json_data[\"train\"]\n",
        "    test = json_data[\"test\"]\n",
        "    \n",
        "\n",
        "# Quick Check\n",
        "counter_train = Counter((x[1] for x in train))\n",
        "counter_test = Counter((x[1] for x in test))\n",
        "print(\"Number of train reviews : \", len(train))\n",
        "print(\"----> # of positive : \", counter_train[1])\n",
        "print(\"----> # of negative : \", counter_train[0])\n",
        "print(\"\")\n",
        "print(train[0])\n",
        "print(\"\")\n",
        "print(\"Number of test reviews : \",len(test))\n",
        "print(\"----> # of positive : \", counter_test[1])\n",
        "print(\"----> # of negative : \", counter_test[0])\n",
        "\n",
        "print(\"\")\n",
        "print(test[0])\n",
        "print(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the Tokenizer"
      ],
      "metadata": {
        "id": "0dsRcmntwfOH"
      },
      "id": "0dsRcmntwfOH"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4381e234",
      "metadata": {
        "id": "4381e234"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment the Tokenizer on the first train review"
      ],
      "metadata": {
        "id": "GPTSFflDwkoh"
      },
      "id": "GPTSFflDwkoh"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ddc98b0c",
      "metadata": {
        "id": "ddc98b0c"
      },
      "outputs": [],
      "source": [
        "# Memory needs\n",
        "# On fait ça car dans pytorch tout va être encodé en tenser qui doivent avoir la même taille\n",
        "# Appliquer un masque pour pas tenir compte des tokens apres 512\n",
        "maxL = 512 # Max length of the sequence\n",
        "\n",
        "\n",
        "# string_tokenized classe batch_encoding \n",
        "string_tokenized = tokenizer.encode_plus(train[0][0], return_tensors=\"pt\", \n",
        "                                        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n",
        "                            max_length=maxL,  # set max length\n",
        "                            truncation=True,  # truncate longer messages\n",
        "                            #pad_to_max_length=True\n",
        "                            padding='max_length',  # add padding\n",
        "                            return_attention_mask=True) # Sert à identifer les tokens masqués ou pas\n",
        "\n",
        "# Retourne l'indice des mots dans le vocabulaire\n",
        "# Et le masque appliqué"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the tokenizer string_tokenized (class BatchEncoding) returns two elements:\n",
        "\n",
        "\n",
        "*   string_tokenized['input_ids']: the index of each token in the dictionary\n",
        "*   string_tokenized['attention_mask']: a binary mask (0 to ignore the token, 1 to consider it). This is because we need tensor a fixed length and we have reviews with a variable number of words\n",
        "\n"
      ],
      "metadata": {
        "id": "v910nrNVx33z"
      },
      "id": "v910nrNVx33z"
    },
    {
      "cell_type": "code",
      "source": [
        "print(string_tokenized['input_ids'])        # Indice des mots dans le dictionnaire\n",
        "print(string_tokenized['attention_mask'])   # On s'en sert parce qu'on a des données de longueur variables"
      ],
      "metadata": {
        "id": "sagULO5nx-3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1df488-8578-47e1-e69c-a4fb78bb027b"
      },
      "id": "sagULO5nx-3H",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1996, 25672, 12083,  3064, 12944,  1997,  2023,  3185,  2003,\n",
            "          2848,  1051,  1005,  6994,  2063,  1005,  1055,  2836,  1012,  1999,\n",
            "          2735, 13544, 29257,  1998, 16668, 16668, 13800,  1012,  2515, 10334,\n",
            "          2079,  2009,  2488,  2084,  1051,  1005,  6994,  2063,  1029,  1045,\n",
            "          2123,  1005,  1056,  2228,  2061,  1012,  2054,  1037,  2307,  2227,\n",
            "          2008,  2158,  2038,   999,  1026,  7987,  1013,  1028,  1026,  7987,\n",
            "          1013,  1028,  1996,  2466,  2003,  2019,  5976,  2028,  1998,  3243,\n",
            "         14888,  1998, 14868,  6387,  1999,  3033,  1006,  2926,  2646,  1996,\n",
            "          2203,  1007,  2021,  2009,  2003,  2036, 15056,  7244,  1998,  2515,\n",
            "          9510,  2006,  2116,  3798,  1012,  2174,  1010,  1045,  2371,  1996,\n",
            "          2143, 10468,  7065, 16116,  2105,  2848,  1051,  1005,  6994,  2063,\n",
            "          1005,  1055, 25567,  2836,  1998,  1045,  1005,  1049,  2469,  1045,\n",
            "          2876,  1005,  1056,  2031,  5632,  2009,  2130,  2431,  2004,  2172,\n",
            "          2065,  2002,  2910,  1005,  1056,  2042,  1999,  2009,  1012,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets download a BERT model for word embedding"
      ],
      "metadata": {
        "id": "TunnUo2p0FT9"
      },
      "id": "TunnUo2p0FT9"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9fc3269c",
      "metadata": {
        "id": "9fc3269c"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
        "\n",
        "# Bert est entrainé pour des taches de masking\n",
        "# masking le mot : supprimer le mot et le reconstruire (predire le mot a partir de son contexte)\n",
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1155599b",
      "metadata": {
        "id": "1155599b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "730dc225-26cd-4dd3-8885-9e305f53d341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "\n",
        "# Couche vector embedding\n",
        "# Couche position embedding\n",
        "# Layer\n",
        "# Encodeur : couche attention, =layer norm, couche dense\n",
        "\n",
        "# Phrase\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can use the BERT model for directly predicting polarity.** Let us apply that on the first review which has been tokenized with string_tokenized."
      ],
      "metadata": {
        "id": "23VAnnrU0QO5"
      },
      "id": "23VAnnrU0QO5"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a15ab0db",
      "metadata": {
        "id": "a15ab0db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae44bc3-7491-4b12-e9dd-73dea5285296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3.1310, -3.5524]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([1, 512, 768])\n",
            "tensor([ 1.0504, -0.3605,  0.2802, -0.3546, -0.0434,  1.2302, -0.3101,  0.0072,\n",
            "        -0.4175], grad_fn=<SliceBackward0>)\n",
            " norm cls token=14.775232315063477\n"
          ]
        }
      ],
      "source": [
        "# Some preliminary test\n",
        "import torch\n",
        "import numpy as np\n",
        "b_input_ids = string_tokenized['input_ids']\n",
        "b_input_mask = string_tokenized['attention_mask']\n",
        "\n",
        "model.eval()\n",
        "\n",
        "output = model(input_ids=b_input_ids,attention_mask=b_input_mask, output_hidden_states=True) # On veut toutes les couches d'activation intermediaire\n",
        "# Score avant la fonction d'activation\n",
        "print(output.logits) # The output of the logit of the two classes (polarity pos/neg)\n",
        "\n",
        "\n",
        "last_hidden_states = output.hidden_states[-1] # The last layer before the class prediction: tensor of size nBatch (1 here) x MaxL (512) x temb (768)\n",
        "print(last_hidden_states.shape) \n",
        "\n",
        "# Tensor\n",
        "# 1 : dire qu'on a un seul exemple (batch)\n",
        "# 512 Nombre de tokens\n",
        "# 768 : dimension de la derniere couche de bert\n",
        "\n",
        "# CLS has 768 dim\n",
        "print(last_hidden_states[0,0,1:10]) # The first 10 value of the first elements (=[CLS] TOKEN)\n",
        "print(f\" norm cls token={np.linalg.norm(last_hidden_states.detach().numpy()[0,0,:])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's tokenize the whole dataset "
      ],
      "metadata": {
        "id": "hdBab99u4HNm"
      },
      "id": "hdBab99u4HNm"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d40ca05a",
      "metadata": {
        "id": "d40ca05a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c23aed-79b4-460b-9f02-e34d548942be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "2500\n",
            "5000\n",
            "7500\n",
            "10000\n",
            "12500\n",
            "15000\n",
            "17500\n",
            "20000\n",
            "22500\n",
            "0\n",
            "2500\n",
            "5000\n",
            "7500\n",
            "10000\n",
            "12500\n",
            "15000\n",
            "17500\n",
            "20000\n",
            "22500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "maxL = 512\n",
        "temb = 768\n",
        "\n",
        "\n",
        "inputs_tokens_train = []\n",
        "attention_masks_train = []\n",
        "\n",
        "for i in range(len(train)):\n",
        "    if(i%2500==0):\n",
        "        print(i)\n",
        "    string_tokenized = tokenizer.encode_plus(train[i][0], return_tensors=\"pt\", \n",
        "                                        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n",
        "                            max_length=maxL,  # set max length\n",
        "                            truncation=True,  # truncate longer messages\n",
        "                            #pad_to_max_length=True\n",
        "                            padding='max_length',  # add padding\n",
        "                            return_attention_mask=True)\n",
        "    \n",
        "    # APPEND inputs token and input masks. YOUR CODE HERE\n",
        "    inputs_tokens_train.append(string_tokenized['input_ids'])\n",
        "    attention_masks_train.append(string_tokenized['attention_mask'])\n",
        "    \n",
        "inputs_tokens_test = []\n",
        "attention_masks_test = []\n",
        "\n",
        "for i in range(len(test)):\n",
        "    if(i%2500==0):\n",
        "        print(i)\n",
        "    string_tokenized = tokenizer.encode_plus(test[i][0], return_tensors=\"pt\", \n",
        "                                        add_special_tokens=True,  # add '[CLS]' and '[SEP]'\n",
        "                            max_length=maxL,  # set max length\n",
        "                            truncation=True,  # truncate longer messages\n",
        "                            #pad_to_max_length=True\n",
        "                            padding='max_length',  # add padding\n",
        "                            return_attention_mask=True)\n",
        "    \n",
        "    # APPEND inputs token and input masks. YOUR CODE HERE\n",
        "    inputs_tokens_test.append(string_tokenized['input_ids'])\n",
        "    attention_masks_test.append(string_tokenized['attention_mask'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's create a 'TensorDataSet' FOR THE TRAINING SAMPLES where each element is a triplet composed of token word index, token mask, and label"
      ],
      "metadata": {
        "id": "lZrSrBuS-HnW"
      },
      "id": "lZrSrBuS-HnW"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "93880db1",
      "metadata": {
        "id": "93880db1"
      },
      "outputs": [],
      "source": [
        "# Converting input tokens to torch tensors \n",
        "# Concatene tte la liste et cree un torch tensor (ressemble à un np.array mais propre a torch)\n",
        "inputs_tokens_train = torch.cat(inputs_tokens_train, dim=0)\n",
        "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
        "\n",
        "\n",
        "# Converting labels to numpy then torch tensor\n",
        "y_train = np.zeros((len(train),))\n",
        "for i in range(len(train)):\n",
        "    y_train[i] = train[i][1]\n",
        "y_train = torch.from_numpy(y_train)\n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "train_dataset = TensorDataset(inputs_tokens_train, attention_masks_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's do the same FOR THE TEST SAMPLES "
      ],
      "metadata": {
        "id": "BpuQLQEm_WSC"
      },
      "id": "BpuQLQEm_WSC"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "88411b6a",
      "metadata": {
        "id": "88411b6a"
      },
      "outputs": [],
      "source": [
        "# Converting input tokens to torch tensors \n",
        "inputs_tokens_test = torch.cat(inputs_tokens_test, dim=0)\n",
        "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
        "  \n",
        "y_test = np.zeros((len(test),))\n",
        "for i in range(len(test)):\n",
        "    y_test[i] = test[i][1]\n",
        "y_test = torch.from_numpy(y_test)\n",
        "\n",
        "test_dataset = TensorDataset(inputs_tokens_test, attention_masks_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you need to clean GPU memory\n",
        "#import gc\n",
        "#gc.collect()\n",
        "#torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "1bX5vtWb_vj8"
      },
      "id": "1bX5vtWb_vj8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most important STEP: we want to extract the [CLS] representation (1st token of the last layer before logits) for each review, and store it in train and test.  "
      ],
      "metadata": {
        "id": "FoDmvAIO_6l8"
      },
      "id": "FoDmvAIO_6l8"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "90ee3dc0",
      "metadata": {
        "id": "90ee3dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f24c26-ec5e-4d57-e0bc-0f8793f96d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nb batches=250\n",
            "batch 0 / 250\n",
            "batch 10 / 250\n",
            "batch 20 / 250\n",
            "batch 30 / 250\n",
            "batch 40 / 250\n",
            "batch 50 / 250\n",
            "batch 60 / 250\n",
            "batch 70 / 250\n",
            "batch 80 / 250\n",
            "batch 90 / 250\n",
            "batch 100 / 250\n",
            "batch 110 / 250\n",
            "batch 120 / 250\n",
            "batch 130 / 250\n",
            "batch 140 / 250\n",
            "batch 150 / 250\n",
            "batch 160 / 250\n",
            "batch 170 / 250\n",
            "batch 180 / 250\n",
            "batch 190 / 250\n",
            "batch 200 / 250\n",
            "batch 210 / 250\n",
            "batch 220 / 250\n",
            "batch 240 / 250\n"
          ]
        }
      ],
      "source": [
        "# create DataLoaders with samplers\n",
        "\n",
        "\n",
        "# Les éléments sont considérés par batch (sous ensemble de données)\n",
        "# Au lieu d'avoir une phrase en entrée, y en aura 100\n",
        "# Tres classique\n",
        "tb = int(100) \n",
        "# Chargement automatique des données (class pytorch)\n",
        "# En pratique, shuffle important\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=tb,shuffle=False)\n",
        "nbTrain = len(train)\n",
        "# feature de train\n",
        "# Pour chaque phrase on va avoir 768 features\n",
        "f_train = np.zeros((nbTrain, temb))\n",
        "nbtach = int(nbTrain/tb)\n",
        "print(f\"nb batches={nbtach}\")\n",
        "# Comuting CLS features\n",
        "# Carte graphique\n",
        "model.cuda()\n",
        "for idx,batch in enumerate(train_dataloader):\n",
        "        # Unpack this training batch from our dataloader:\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        if(idx%10==0):\n",
        "            print(f\"batch {idx} / {nbtach}\")\n",
        "\n",
        "        # Basculer vers la mémoire graphique de façon transparente\n",
        "        b_input_ids = batch[0].cuda()\n",
        "        b_input_mask = batch[1].cuda()\n",
        "        b_labels = batch[2].cuda().long()\n",
        "        \n",
        "        # On cherche pas à apprendre les paramètres du modèle\n",
        "        # Pas de gradient ni de backprop\n",
        "        with torch.no_grad():\n",
        "            # forward propagation (evaluate model on training batch)\n",
        "            output = model(input_ids=b_input_ids,\n",
        "                                 attention_mask=b_input_mask,\n",
        "                                 #labels=b_labels, \n",
        "                               output_hidden_states=True)\n",
        "            last_hidden_states = output.hidden_states[-1] # WARNING: it is now a batch of size tbatch x nToken x embsize \n",
        "\n",
        "            # Récupérer tout les cls\n",
        "            # .detach().cpu().numpy()\n",
        "            f_train[idx*tb:idx*tb+tb,:] = last_hidden_states[:,0,:].detach().cpu().numpy() # YOUR CODE HERE. Think in applying .detach().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the features and labels\n",
        "import pickle\n",
        "# Open a file and use dump()\n",
        "with open('train-data.pkl', 'wb') as file:\n",
        "    # A new file will be created\n",
        "    pickle.dump([f_train,y_train], file)"
      ],
      "metadata": {
        "id": "qBb2wIpXwt17"
      },
      "id": "qBb2wIpXwt17",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract [CLS] token in TEST"
      ],
      "metadata": {
        "id": "srd5tJqyJSRq"
      },
      "id": "srd5tJqyJSRq"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "0aafb30d-357b-46da-a1f8-bfd18832ba24",
      "metadata": {
        "id": "0aafb30d-357b-46da-a1f8-bfd18832ba24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2517db-058c-4f95-a6c6-71b16bf66c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nb batches=250\n",
            "batch 0 / 250\n",
            "batch 10 / 250\n",
            "batch 20 / 250\n",
            "batch 30 / 250\n",
            "batch 40 / 250\n",
            "batch 50 / 250\n",
            "batch 60 / 250\n",
            "batch 70 / 250\n",
            "batch 80 / 250\n",
            "batch 90 / 250\n",
            "batch 100 / 250\n",
            "batch 110 / 250\n",
            "batch 120 / 250\n",
            "batch 130 / 250\n",
            "batch 140 / 250\n",
            "batch 150 / 250\n",
            "batch 160 / 250\n",
            "batch 170 / 250\n",
            "batch 180 / 250\n",
            "batch 190 / 250\n",
            "batch 200 / 250\n",
            "batch 210 / 250\n",
            "batch 220 / 250\n",
            "batch 230 / 250\n",
            "batch 240 / 250\n"
          ]
        }
      ],
      "source": [
        "# create DataLoaders with samplers\n",
        "tb = int(100)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=tb,shuffle=False)\n",
        "nbTest = len(test)\n",
        "f_test = np.zeros((nbTest, temb))\n",
        "nbtach = int(nbTest/tb)\n",
        "print(f\"nb batches={nbtach}\")\n",
        "# Comuting CLS features\n",
        "model.cuda()\n",
        "for idx,batch in enumerate(test_dataloader):\n",
        "        # Unpack this training batch from our dataloader:\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        if(idx%10==0):\n",
        "            print(f\"batch {idx} / {nbtach}\")\n",
        "        b_input_ids = batch[0].cuda()\n",
        "        b_input_mask = batch[1].cuda()\n",
        "        b_labels = batch[2].cuda().long()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # forward propagation (evaluate model on training batch)\n",
        "            output = model(input_ids=b_input_ids,\n",
        "                                 attention_mask=b_input_mask,\n",
        "                                 #labels=b_labels, \n",
        "                               output_hidden_states=True)\n",
        "            last_hidden_states = output.hidden_states[-1] # YOUR CODE HERE.\n",
        "            #\n",
        "            f_test[idx*tb:idx*tb+tb,:] = last_hidden_states[:,0,:].detach().cpu().numpy() # YOUR CODE HERE.\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now save the embedding of each review into disk!"
      ],
      "metadata": {
        "id": "AReISeiZIo9U"
      },
      "id": "AReISeiZIo9U"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "deb81156-d885-4999-a6e7-2d8149173174",
      "metadata": {
        "id": "deb81156-d885-4999-a6e7-2d8149173174"
      },
      "outputs": [],
      "source": [
        "# Saving the features and labels\n",
        "import pickle\n",
        "\n",
        "with open('test-data.pkl', 'wb') as file:\n",
        "    # A new file will be created\n",
        "    pickle.dump([f_test,y_test], file)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "4104eea1",
      "metadata": {
        "id": "4104eea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc84d95-fd5f-49b1-e1b5-dd09ad6c7743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "(25000, 768)\n",
            "tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64)\n",
            "tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64)\n",
            "15.125989001774101\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(f_train.shape[0])\n",
        "print(f_test.shape)\n",
        "\n",
        "print(y_train)\n",
        "print(y_test)\n",
        "print(np.linalg.norm(f_train[10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally: train a logistic regression model on top of extracted embeddings. Conclude on the performances of BERT for the sentiment classification task"
      ],
      "metadata": {
        "id": "mx0eec9RIwJr"
      },
      "id": "mx0eec9RIwJr"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Train a logistic regression model\n",
        "clf = LogisticRegression(max_iter=500)\n",
        "clf.fit(f_train, y_train)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "accuracy = clf.score(f_test, y_test)\n",
        "\n",
        "# Results\n",
        "print(\"Train accuracy:\", clf.score(f_train, y_train))\n",
        "print(\"Test accuracy:\", clf.score(f_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkckB6XL2us7",
        "outputId": "e01db32e-7d69-4292-97d1-d571d5721a60"
      },
      "id": "xkckB6XL2us7",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.89284\n",
            "Test accuracy: 0.89012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec : Moyennage pur fait diminuer les perf\n",
        "Bert     : un peu plus\n",
        "\n",
        "Classif de sentiments : Plus fins"
      ],
      "metadata": {
        "id": "-QNbrAcu4kPv"
      },
      "id": "-QNbrAcu4kPv"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3wq8oeA5Ler"
      },
      "id": "g3wq8oeA5Ler",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}